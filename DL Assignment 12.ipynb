{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0284e925-a924-415f-a6fb-6fc12f15867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "# Ans:\n",
    "# The unsqueeze function in PyTorch helps solve certain broadcasting problems by adding a new dimension to a tensor.\n",
    "\n",
    "# By using unsqueeze, you can increase the number of dimensions of a tensor at a specified position. This can be useful when aligning\n",
    "# tensors for broadcasting, especially when dealing with tensors of different shapes.\n",
    "\n",
    "# For example, if you have a 1-dimensional tensor a of shape (3,) and you want to perform elementwise operations with a 2-dimensional\n",
    "# tensor b of shape (3, 4), you can use unsqueeze to add a new dimension to a to match the shape of b. The new shape of a would become'\n",
    "# (3, 1), allowing it to align with b for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f2ac61-a46a-4997-b613-8ab94a63cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. How can we use indexing to do the same operation as unsqueeze?\n",
    "\n",
    "# Ans:\n",
    "# We can use indexing with None or np.newaxis to achieve the same operation as unsqueeze in PyTorch or NumPy, respectively.\n",
    "\n",
    "# By inserting None or np.newaxis at a specific index while indexing a tensor, we can effectively add a new dimension to the tensor \n",
    "# at that position. This helps in aligning tensors for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b0d1ef-96e9-478f-811a-5e4485af28af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.7)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.3)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "[1, 2, 3, 4, 5]\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# 3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "# Sol:\n",
    "!pip install torch\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Print the contents of the memory as a list\n",
    "print(tensor.tolist())\n",
    "\n",
    "# Print the contents of the memory as a NumPy array\n",
    "print(tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2878688-d715-4929-8b0b-a4d09593c925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  7,  9],\n",
      "        [ 8, 10, 12],\n",
      "        [11, 13, 15]])\n"
     ]
    }
   ],
   "source": [
    "# 4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "# to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "\n",
    "# Sol:\n",
    "\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.tensor([[4, 5, 6],\n",
    "                       [7, 8, 9],\n",
    "                       [10, 11, 12]])\n",
    "\n",
    "result = vector + matrix\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b941212c-ab59-4619-b440-5d444ddc0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "\n",
    "# Ans:\n",
    "# No, broadcasting and expand_as operations do not result in increased memory use.\n",
    "\n",
    "# Both broadcasting and expand_as are memory-efficient operations that avoid unnecessary memory allocation. They operate on the original \n",
    "# tensors without creating additional copies or allocating new memory for the expanded or broadcasted tensors. Instead, they utilize\n",
    "# memory sharing and indexing mechanisms to enable elementwise operations on tensors with different shapes.\n",
    "\n",
    "# This memory efficiency is achieved by leveraging the underlying memory layout of the original tensors and performing the operations \n",
    "# on the fly, only when required. As a result, broadcasting and expand_as allow for efficient computation and memory utilization without\n",
    "# incurring additional memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b4102a-b573-40a9-b3d0-f95edc3082c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# 6. Implement matmul using Einstein summation.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def matmul_einsum(a, b):\n",
    "    return np.einsum('ij, jk -> ik', a, b)\n",
    "\n",
    "# Example usage\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "result = matmul_einsum(a, b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec26046f-abbb-466d-a12a-723987deff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "\n",
    "# Ans:\n",
    "# In Einstein summation notation, a repeated index letter on the left-hand side of einsum represents a summation or contraction\n",
    "# operation over that index.\n",
    "\n",
    "# When an index letter is repeated on the left-hand side, it indicates that the corresponding dimensions of the input arrays will be \n",
    "# multiplied elementwise, and then summed over that index.\n",
    "\n",
    "# For example, in the Einstein summation notation 'ij, jk -> ik', the repeated index letter j indicates a summation or contraction \n",
    "# operation. It means that the elements along the shared dimension j will be multiplied and summed, resulting in a new array with \n",
    "# dimensions specified by the remaining non-repeated indices, i and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2df3d1e-92bd-49ea-854c-29e947bb66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "# Ans:\n",
    "# The three rules of Einstein summation notation are:\n",
    "\n",
    "# Repeated Index: When an index appears twice in an expression (e.g., 'ij, jk -> ik'), it implies a summation or contraction over that \n",
    "# index.\n",
    "\n",
    "# Index Ordering: The order of indices matters. The order on the right-hand side of the '->' arrow determines the order of indices on \n",
    "# the left-hand side.\n",
    "\n",
    "# Index Range: Each index must have a defined range or size corresponding to the dimensions of the input arrays. The range of an index\n",
    "# is determined by the size of the corresponding dimension of the input arrays involved in the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5409bd-d059-4974-8416-523094c6dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "# Ans:\n",
    "# The forward pass of a neural network refers to the process of propagating input data through the network's layers, from the input \n",
    "# layer to the output layer. During the forward pass, each layer performs its computations, applying activation functions and weight \n",
    "# operations, to generate an output prediction or representation.\n",
    "\n",
    "# The backward pass, also known as backpropagation, is the process of computing gradients of the network's parameters with respect to a \n",
    "# loss function. It involves propagating the error or loss backward through the network, updating the weights and biases of each layer\n",
    "# based on the computed gradients using optimization algorithms such as gradient descent. The backward pass allows the network to learn \n",
    "# and adjust its parameters to minimize the loss and improve its predictions over time through gradient-based optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7e908f9-fa47-489e-8d9f-7345da0d3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "\n",
    "# Ans:\n",
    "# Storing activations calculated for intermediate layers in the forward pass is necessary because they are needed for the backward pass\n",
    "# during backpropagation.\n",
    "\n",
    "# During backpropagation, gradients are computed by propagating the error backwards through the network. This process requires the\n",
    "# intermediate activations, which are the outputs of the layers during the forward pass, to compute the gradients of the loss with \n",
    "# respect to the weights and biases of each layer.\n",
    "\n",
    "# By storing the intermediate activations, we can efficiently compute the gradients and update the network's parameters. These activations\n",
    "# serve as crucial information for calculating the gradients accurately and efficiently, ensuring effective learning and optimization of\n",
    "# the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdcc318c-db65-4499-a491-830dad8a0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "# Ans:\n",
    "# The downside of having activations with a standard deviation too far away from 1 is that it can lead to issues with the optimization \n",
    "# process during training.\n",
    "\n",
    "# If the standard deviation of the activations is too high, it can cause exploding gradients, where the gradients become extremely large.\n",
    "# This can result in unstable training, making it difficult for the network to converge to an optimal solution. It can lead to erratic \n",
    "# updates of the network's parameters and slower convergence or even divergence of the training process.\n",
    "\n",
    "# On the other hand, if the standard deviation of the activations is too low, it can cause vanishing gradients, where the gradients become\n",
    "# extremely small. This can hinder the flow of gradient information backward through the network during backpropagation. The network may\n",
    "# struggle to learn effectively, especially in deeper architectures, as the gradients diminish rapidly with each layer, leading to slower\n",
    "# or no learning.\n",
    "\n",
    "# Therefore, it is desirable to have activations with a standard deviation close to 1, as it helps maintain a reasonable magnitude of \n",
    "# gradients during training, leading to stable and efficient optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82976f6e-44d8-4828-885b-8c8180d52d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. How can weight initialization help avoid this problem?\n",
    "\n",
    "# Ans:\n",
    "# Weight initialization can help avoid the problem of activations with a standard deviation too far from 1 by setting an appropriate \n",
    "# initial range for the weights.\n",
    "\n",
    "# By initializing the weights of the neural network properly, we can promote activations that are neither too high nor too low in\n",
    "# magnitude. This can help prevent the issues of exploding or vanishing gradients during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
