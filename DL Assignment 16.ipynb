{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c9efc4-ac52-4f51-b539-bc71a68c7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Explain the Activation Functions in your own language\n",
    "# a) sigmoid\n",
    "# b) tanh\n",
    "# c) ReLU\n",
    "# d) ELU\n",
    "# e) LeakyReLU\n",
    "# f) swish\n",
    "\n",
    "# Ans:\n",
    "# a) Sigmoid: The sigmoid activation function squeezes the input values between 0 and 1, mapping negative values to close to 0 and \n",
    "# positive values to close to 1.\n",
    "\n",
    "# b) Tanh: The tanh activation function is similar to the sigmoid function but ranges between -1 and 1, mapping negative values to close \n",
    "# to -1 and positive values to close to 1.\n",
    "\n",
    "# c) ReLU (Rectified Linear Unit): The ReLU activation function sets all negative input values to 0 and leaves positive values unchanged.\n",
    "\n",
    "# d) ELU (Exponential Linear Unit): The ELU activation function is similar to ReLU for positive values but smoothly approximates negative\n",
    "# values to avoid the \"dead neuron\" problem.\n",
    "\n",
    "# e) LeakyReLU: The LeakyReLU activation function is a variation of ReLU that allows a small slope for negative values, preventing the \n",
    "# complete \"death\" of neurons.\n",
    "\n",
    "# f) Swish: The Swish activation function is a smooth and continuously differentiable function that performs better than ReLU in some \n",
    "# cases by providing a non-monotonic but well-behaved activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace79e07-e525-4472-9d58-27ad007c77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "# Ans:\n",
    "# When you increase the optimizer learning rate, the model converges faster during training, but there is a higher risk of \n",
    "# overshooting the optimal solution and ending up with unstable or divergent results.\n",
    "\n",
    "# When you decrease the optimizer learning rate, the model converges more slowly during training, but there is a lower risk of\n",
    "# overshooting the optimal solution. However, if the learning rate is too small, the model may take a long time to converge or \n",
    "# get stuck in a suboptimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296ad7d5-ef67-4f99-8de6-9c87189e76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "# Ans:\n",
    "# When you increase the number of internal hidden neurons in a neural network, it increases the model's capacity to learn complex patterns\n",
    "# and representations from the data. This can potentially improve the model's ability to capture intricate relationships and increase \n",
    "# its overall performance. However, increasing the number of hidden neurons also increases the model's computational complexity and \n",
    "# memory requirements. There is a trade-off between model capacity and computational efficiency, so it's important to find an optimal \n",
    "# balance based on the specific problem and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2126db-3771-47a4-b294-5dfc60727ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What happens when you increase the size of batch computation?\n",
    "\n",
    "# Ans:\n",
    "# When you increase the size of batch computation (i.e., increase the batch size), it can lead to faster training times and more\n",
    "# efficient parallel processing, as more training examples are processed simultaneously. Additionally, larger batch sizes can \n",
    "# provide more stable gradients and better generalization, especially when the training dataset is noisy. However, increasing the \n",
    "# batch size also requires more memory to store the intermediate computations, and it may result in slower convergence and potentially \n",
    "# poorer local optima in some cases. The choice of batch size should be carefully considered based on the specific problem, available \n",
    "# computational resources, and desired trade-offs between training efficiency and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241a74bf-af51-4aaf-af8c-c61a23eaebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "# Ans:\n",
    "# We adopt regularization techniques to avoid overfitting by introducing additional constraints or penalties to the model during training.\n",
    "# Regularization helps to reduce the model's reliance on individual training examples or specific features, encouraging it to generalize\n",
    "# better to unseen data. It prevents the model from fitting the noise or idiosyncrasies in the training data, leading to better \n",
    "# performance on the test data and reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8616010f-d5c4-4888-a4ee-002203cef96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What are loss and cost functions in deep learning?\n",
    "\n",
    "# Ans:\n",
    "# Loss functions and cost functions are both used in deep learning to quantify the error or discrepancy between the predicted outputs \n",
    "# of a neural network and the actual target values. The loss function is typically used during the training process to measure the error\n",
    "# for a single training example, while the cost function is the average or cumulative loss over the entire training set. The objective is\n",
    "# to minimize the loss or cost function, which drives the network to learn and improve its predictions. Different types of loss functions,\n",
    "# such as mean squared error (MSE) or cross-entropy, can be used depending on the specific task and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe11283-a1a2-4f58-8fa6-ba43649f751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. What do ou mean by underfitting in neural networks?\n",
    "\n",
    "# Ans:\n",
    "# Underfitting in neural networks refers to a situation where the model is unable to capture the underlying patterns or relationships in \n",
    "# the training data effectively. It occurs when the model is too simple or lacks the necessary complexity to adequately represent the data. \n",
    "# As a result, the model's performance on both the training and test data is suboptimal, with high bias and low variance. Underfitting \n",
    "# can be addressed by increasing the model's capacity, such as adding more layers or neurons, or using more advanced architectures to\n",
    "# capture the complexity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a101fd-6fe6-431c-a75f-48718741d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Why we use Dropout in Neural Networks?\n",
    "\n",
    "# Ans:\n",
    "# Dropout is used in neural networks as a regularization technique to prevent overfitting. It randomly sets a fraction of the neuron's\n",
    "# inputs to zero during each training step, which helps in reducing the interdependence among neurons. This, in turn, encourages the \n",
    "# network to learn more robust and generalized representations of the data. Dropout effectively acts as an ensemble method by training \n",
    "# multiple subnetworks that share parameters, which improves the network's ability to handle noise, generalize to unseen data, \n",
    "# and mitigate the effects of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e257a-3c23-4ecc-ba53-379749b723ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
