{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec4d129-db11-4b03-a278-159c06a7c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "# Ans:\n",
    "# No, it is not recommended to initialize all weights to the same value, even if it is chosen randomly using He initialization. \n",
    "# Weight symmetry can occur when all weights are the same, which leads to the inability of neurons to learn different features.\n",
    "# It is important to introduce diversity in weight initialization to promote effective learning and prevent symmetry problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccddb5b-2b68-4856-b01e-2fc333ef0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "# Ans:\n",
    "# Yes, it is generally acceptable to initialize the bias terms to 0. The bias term represents the offset or baseline value for each \n",
    "# neuron and does not need to be initialized with a specific value as long as the weights are appropriately initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4a3aa1-8aff-4887-b4da-7bd85ea77522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "# Ans:\n",
    "# SELU activation can self-normalize, leading to stable and well-behaved gradients during training.\n",
    "# SELU can alleviate the vanishing/exploding gradient problem, allowing for deeper networks to be trained effectively.\n",
    "# SELU has a negative saturation region, which helps capture negative correlations and enables improved performance on \n",
    "# certain types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f46713-2c4e-4fe7-a20a-07e57301214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "# ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "# Ans:\n",
    "# SELU: SELU activation function is suitable for deep neural networks as it helps in stabilizing gradients and enables self-normalization.\n",
    "# It is recommended when dealing with complex and deep architectures.\n",
    "# Leaky ReLU (and its variants): Leaky ReLU is useful when preventing the \"dying ReLU\" problem, which occurs when ReLU units\n",
    "# become inactive during training. It can be used as an alternative to ReLU to introduce a small negative slope for negative inputs,\n",
    "# promoting better gradient flow.\n",
    "# ReLU: ReLU is commonly used in many deep learning applications as it helps address the vanishing gradient problem and accelerates \n",
    "# training. It is suitable for most hidden layers in deep neural networks.\n",
    "# tanh: tanh activation function is often used in recurrent neural networks (RNNs) or when working with data that has negative values.\n",
    "# It squashes the input to the range [-1, 1], making it useful for capturing both positive and negative correlations in the data.\n",
    "# logistic (sigmoid): The logistic activation function (sigmoid) is commonly used in binary classification problems or when outputs \n",
    "# need to be interpreted as probabilities. It maps inputs to a range between 0 and 1, enabling probabilistic interpretations.\n",
    "# softmax: Softmax activation function is used in multi-class classification problems, where the goal is to assign probabilities\n",
    "# to each class. It normalizes the output values to sum up to 1, allowing for easy interpretation as class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5af3f4e-2e5c-45a5-9ecc-34eb7f6f26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "# Ans:\n",
    "# Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) in an SGD optimizer can cause the model to have slow convergence or\n",
    "# even result in instability during training. The excessive momentum can cause the optimizer to overshoot the optimal solution and \n",
    "# hinder the ability to converge effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d14033-381e-4716-b939-8727129db2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Name three ways you can produce a sparse model.\n",
    "\n",
    "# Ans:\n",
    "# Three ways to produce a sparse model are:\n",
    "\n",
    "# L1 regularization: Applying L1 regularization to the model's loss function encourages sparsity by promoting many weights to \n",
    "# become exactly zero.\n",
    "# Dropout: Randomly setting a fraction of the neurons or their activations to zero during training can induce sparsity in the network.\n",
    "# Pruning: Removing connections with small weights or neurons with low activations can create a sparse model by eliminating unnecessary\n",
    "# parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee87d930-a2fc-4e72-8a98-bdb1b33f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "# Ans:\n",
    "# Dropout can slightly slow down training since it requires additional computations to randomly drop out units during each training\n",
    "# iteration. However, dropout does not typically significantly impact inference speed as it is not applied during inference.\n",
    "\n",
    "# MC Dropout (Monte Carlo Dropout) can slow down inference since it involves performing multiple forward passes with dropout enabled \n",
    "# to obtain predictions with uncertainty estimates. The increased computation time can be a trade-off for obtaining more reliable\n",
    "# predictions and uncertainty information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2efc092-6760-4c6e-9dcc-7eacb178e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "# a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "# point of this exercise). Use He initialization and the ELU activation function.\n",
    "# b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "# dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "# composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for \n",
    "# testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "# Remember to search for the right learning rate each time you change the model’s\n",
    "# architecture or hyperparameters.\n",
    "#  c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "# converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "# d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "# to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "# LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "# e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "# see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "# Ans:\n",
    "# a. Building a DNN: The first step is to construct a deep neural network with 20 hidden layers, each consisting of 100 neurons. \n",
    "# He initialization is used to initialize the weights, and the ELU activation function is applied to introduce non-linearity.\n",
    "\n",
    "# b. Training with Nadam optimization and early stopping: The network is trained using the CIFAR10 dataset, which contains 60,000 \n",
    "# 32x32-pixel color images with 10 different classes. Nadam optimization is utilized as the optimizer, and early stopping is \n",
    "# implemented to monitor the validation loss and halt training if there is no improvement.\n",
    "\n",
    "# c. Adding Batch Normalization: Batch Normalization, a technique that normalizes the inputs of each layer during training, \n",
    "# is added to the network. The learning curves are compared to observe if the model converges faster and produces better results. \n",
    "# Additionally, the impact on training speed is assessed.\n",
    "\n",
    "# d. Replacing Batch Normalization with SELU: Batch Normalization is replaced with SELU (Scaled Exponential Linear Unit) \n",
    "# activation function, which allows for self-normalization of the network. Adjustments are made to ensure the network is self-normalizing,\n",
    "# such as standardizing input features and using LeCun normal initialization.\n",
    "\n",
    "# e. Regularizing with alpha dropout and MC Dropout: The model is regularized using alpha dropout, a type of dropout that retains mean \n",
    "# and variance information. Afterward, the model is evaluated using MC Dropout, where multiple forward passes with dropout enabled\n",
    "# are performed to obtain more accurate predictions and uncertainty estimates.\n",
    "\n",
    "# By following these steps and experimenting with different techniques, it is possible to explore the effects of various strategies\n",
    "# on the model's performance, convergence speed, and accuracy on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95407944-e285-4191-bedd-f2335b5968fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
