{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b097b158-2c75-433f-901e-a7981bf55cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What does a SavedModel contain? How do you inspect its content?\n",
    "\n",
    "# Ans:\n",
    "# A SavedModel contains the TensorFlow model's architecture, weights, and computation graph. It also includes the necessary metadata\n",
    "# for serving the model. You can inspect the content of a SavedModel using the saved_model_cli command-line tool or by programmatically \n",
    "# loading and exploring its components using TensorFlow APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1a3eab-7917-4db6-a34b-8515d3f74713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "\n",
    "# Ans:\n",
    "# You should use TensorFlow Serving when you need to serve trained TensorFlow models in a production environment. \n",
    "# Its main features include high-performance model serving, support for serving multiple models simultaneously, and RESTful API and \n",
    "# gRPC endpoints for model inference. Some tools you can use to deploy TensorFlow Serving include Docker, Kubernetes, and \n",
    "# TensorFlow Serving's built-in ModelServer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3897a1a4-9fb7-4b46-bbf5-64cea89b49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How do you deploy a model across multiple TF Serving instances?\n",
    "\n",
    "# Ans:\n",
    "# To deploy a model across multiple TensorFlow Serving instances, you can use a load balancer or a service orchestrator,\n",
    "# such as Kubernetes. The load balancer distributes the incoming requests across the instances, while the service orchestrator manages \n",
    "# the deployment and scaling of the instances. This allows you to distribute the load and improve the availability and scalability\n",
    "# of your model serving infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50021db-8f73-42f6-85a4-19a0005f7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "\n",
    "# Ans:\n",
    "# You should use the gRPC API rather than the REST API when you require lower latency and higher throughput for querying a model \n",
    "# served by TensorFlow Serving. gRPC offers a more efficient communication protocol compared to REST, making it suitable for \n",
    "# scenarios that demand faster and more efficient interactions with the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47773392-522c-452a-a2a5-e2bb637e0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?\n",
    "\n",
    "# Ans:\n",
    "# TFLite reduces a model's size for deployment on mobile or embedded devices through techniques such as quantization, \n",
    "# weight pruning, and model compression. Quantization reduces the precision of model weights, while weight pruning removes \n",
    "# unnecessary connections. Model compression further reduces the size by employing techniques like Huffman coding\n",
    "# or knowledge distillation. These techniques collectively enable models to be efficiently executed on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aaa2166-9a64-4609-8cbb-52bb4103371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What is quantization-aware training, and why would you need it?\n",
    "\n",
    "# Ans:\n",
    "# Quantization-aware training is a technique used to train models that are aware of the quantization process. \n",
    "# It involves simulating the effects of quantization during training, allowing the model to adapt and maintain performance even with\n",
    "# reduced precision. Quantization-aware training is necessary when deploying models to hardware platforms that require quantized \n",
    "# operations for efficient execution, such as mobile or embedded devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a24772-9193-440a-ac93-a47f3bd9ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "\n",
    "# Ans:\n",
    "# Model parallelism and data parallelism are techniques used in distributed training of deep learning models.\n",
    "\n",
    "# Model parallelism: In model parallelism, different parts or layers of the model are assigned to different devices or machines for \n",
    "# computation. Each device or machine processes a portion of the model independently and exchanges information during training.\n",
    "\n",
    "# Data parallelism: In data parallelism, multiple copies of the model are created on different devices or machines, and each copy \n",
    "# processes a different subset of the training data. The gradients from each device or machine are averaged to update the model parameters.\n",
    "\n",
    "# Data parallelism is generally recommended because it allows for easier implementation and scalability. It distributes the \n",
    "# computational load across multiple devices or machines by processing different training examples simultaneously, resulting in \n",
    "# faster training. Model parallelism, on the other hand, can be more complex to implement and may have limitations in terms of \n",
    "# scalability and communication overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c66b3c9f-775c-4065-bd38-5361023f2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. When training a model across multiple servers, what distribution strategies can you use?\n",
    "# How do you choose which one to use?\n",
    "\n",
    "# Ans:\n",
    "# When training a model across multiple servers, you can use various distribution strategies such as data parallelism, model parallelism, \n",
    "# and hybrid parallelism.\n",
    "\n",
    "# Data parallelism: Each server processes a subset of the training data and synchronizes the model parameters periodically.\n",
    "# Model parallelism: Different servers are responsible for different parts or layers of the model, and they exchange information during \n",
    "# training.\n",
    "# Hybrid parallelism: Combination of data parallelism and model parallelism, where both the data and the model are distributed across\n",
    "# multiple servers.\n",
    "# The choice of distribution strategy depends on factors like the size of the model, available computational resources, \n",
    "# communication overhead, and scalability requirements. Data parallelism is commonly used due to its simplicity, scalability, \n",
    "# and efficient utilization of resources. However, for very large models or specific architectural requirements, model parallelism \n",
    "# or hybrid parallelism may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aa07b-1cdf-4859-b32f-b89f3df9a08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
