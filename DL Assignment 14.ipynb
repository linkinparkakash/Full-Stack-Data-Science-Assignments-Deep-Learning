{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bb0bf9-644a-4693-9143-db22215bb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
    "# randomly using He initialization?\n",
    "\n",
    "# Ans:\n",
    "# No, it is not okay to initialize all the weights to the same value, even if that value is selected randomly using He initialization. \n",
    "# The purpose of weight initialization is to introduce diversity and asymmetry in the network's parameters, enabling it to learn\n",
    "# effectively. Initializing all weights to the same value would eliminate this diversity and hinder the learning process. Therefore, \n",
    "# it is crucial to initialize weights randomly and not set them to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded991e0-1637-4f7c-bc78-dda34c0346ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is it okay to initialize the bias terms to 0?\n",
    "\n",
    "# Ans:\n",
    "# Yes, it is generally okay to initialize the bias terms to 0. Unlike weight initialization, which is crucial for introducing diversity\n",
    "# and breaking symmetry in the network, initializing the biases to 0 does not have a significant impact on the network's learning capacity.\n",
    "# The network can still learn and adjust the biases during the training process to accommodate the desired outputs. However, there may be\n",
    "# cases where non-zero bias initialization can be beneficial, such as when dealing with imbalanced datasets or specific network\n",
    "# architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7a4e9a-7011-4ae8-83fb-5ad19775adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Name three advantages of the ELU activation function over ReLU.\n",
    "\n",
    "# Ans:\n",
    "# Three advantages of the ELU (Exponential Linear Unit) activation function over ReLU (Rectified Linear Unit) are:\n",
    "\n",
    "# Handles negative values: ELU can handle negative input values gracefully, avoiding the \"dying ReLU\" problem where ReLU neurons can \n",
    "# become permanently inactive if they receive negative inputs.\n",
    "\n",
    "# Smoother activation: ELU provides a smooth transition for both positive and negative input values, which can help alleviate issues\n",
    "# related to gradient instability and promote more stable and faster convergence during training.\n",
    "\n",
    "# Supports negative saturation avoidance: ELU prevents saturation of neuron outputs for negative inputs, allowing the network to \n",
    "# learn more diverse and nuanced representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425d382d-2d3f-4174-92d2-f98bf8959117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
    "# ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "# Ans:\n",
    "# ELU: ELU is useful when you want to handle negative values gracefully, prevent dying ReLU problem, and promote faster convergence during\n",
    "# training.\n",
    "\n",
    "# Leaky ReLU (and its variants): Leaky ReLU is suitable when you want to address the dying ReLU problem by introducing a small slope\n",
    "# for negative inputs, allowing the neurons to remain active even for negative values.\n",
    "\n",
    "# ReLU: ReLU is widely used when you want a simple and computationally efficient activation function. It works well in many cases, \n",
    "# especially for hidden layers, where sparsity and non-linearity are desired.\n",
    "\n",
    "# tanh: tanh is useful when you need an activation function that outputs values between -1 and 1, which can be helpful for tasks where \n",
    "# you want both positive and negative activations.\n",
    "\n",
    "# logistic: logistic (also known as sigmoid) is commonly used in binary classification problems where you want to predict probabilities \n",
    "# between 0 and 1, representing the likelihood of a certain class.\n",
    "\n",
    "# softmax: softmax is typically used in the output layer for multi-class classification tasks, where you want to obtain a probability \n",
    "# distribution over multiple classes. It ensures that the sum of the probabilities adds up to 1, allowing easy interpretation of the \n",
    "# outputs as class probabilities.\n",
    "\n",
    "# The choice of activation function depends on the specific problem, network architecture, and desired properties of the activations,\n",
    "# such as non-linearity, range of values, and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca536811-17d5-4193-8dc7-c9f576fe44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "\n",
    "# Ans:\n",
    "# Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer can lead to overshooting or \n",
    "# oscillations during optimization. The high momentum value causes the optimizer to rely heavily on previous gradient updates, \n",
    "# leading to a persistent and exaggerated influence of past gradients on the current update. This can result in unstable and erratic \n",
    "# behavior, making the optimization process less effective and potentially slowing down convergence or even preventing it altogether. \n",
    "# It is generally recommended to use a moderate momentum value (e.g., 0.9) that balances the influence of past gradients with the current \n",
    "# gradients for stable and efficient optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "383ce85b-30e6-4d9d-80f0-65de61ebd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Name three ways you can produce a sparse model.\n",
    "\n",
    "# Three ways to produce a sparse model are:\n",
    "\n",
    "# L1 Regularization: By adding an L1 regularization term to the loss function during training, it encourages the model to minimize the\n",
    "# weights of less important features, resulting in sparsity where irrelevant features have near-zero weights.\n",
    "\n",
    "# Pruning: Pruning involves removing connections or weights with low magnitudes from the trained model. This process can be based on \n",
    "# certain threshold values or criteria such as magnitude, importance, or sensitivity analysis.\n",
    "\n",
    "# Quantization: Quantization involves reducing the precision of weights or activations, such as converting floating-point values to\n",
    "# lower bit representations (e.g., from 32-bit to 8-bit). This reduction in precision can lead to sparsity in the model by clustering \n",
    "# values around a smaller set of representable values, effectively reducing the number of distinct values used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e41b3ee-e5fe-4f5f-93f0-c1ff3d01d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "\n",
    "# Ans:\n",
    "# Yes, dropout can slow down training to some extent. During training, dropout randomly sets a portion of the neurons' outputs to zero,\n",
    "# which effectively introduces noise and encourages the network to learn more robust and generalized representations. This stochastic \n",
    "# nature of dropout requires additional computations and can slightly slow down the training process compared to models without dropout.\n",
    "\n",
    "# However, during inference or making predictions on new instances, dropout is typically turned off or scaled down. In this case, dropout \n",
    "# does not slow down inference since all neurons are active, and no random zeroing of outputs occurs. The network effectively uses the \n",
    "# learned weights and biases without the dropout noise, resulting in faster inference compared to training with dropout enabled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
