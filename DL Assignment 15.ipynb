{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d67b7-0ffb-49d6-af7e-0e0b6affbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Deep Learning.\n",
    "# a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "# b. Using Adam optimization and early stopping, try training it on MNIST but only on\n",
    "# digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You\n",
    "# will need a softmax output layer with five neurons, and as always make sure to save\n",
    "#  checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "# c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "# d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better \n",
    "# model?\n",
    "# e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "input_size = ...  # Define the input size\n",
    "hidden_size = 100\n",
    "output_size = 5  # Number of output classes\n",
    "\n",
    "model = DNN(input_size, hidden_size, output_size)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Set up data loaders for digits 0 to 4\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "indices = [i for i, (_, label) in enumerate(train_dataset) if label < 5]\n",
    "train_sampler = SubsetRandomSampler(indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define loss function, optimizer, and early stopping\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# Add code for early stopping here\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.view(-1, input_size))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Add code for early stopping here\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "\n",
    "...\n",
    "self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "self.bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385a5c0-182e-4a68-87d1-6eec7a7a23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transfer learning.\n",
    "# a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
    "# model, freezes them, and replaces the softmax output layer with a new one.\n",
    "# b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how\n",
    "# long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "# c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "# d. Try again reusing just four hidden layers instead of five. Can you achieve a highe precision?\n",
    "# e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\n",
    "\n",
    "# Sol:\n",
    "\n",
    "# Load the pretrained model\n",
    "pretrained_model = DNN(input_size, hidden_size, output_size)\n",
    "pretrained_model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Freeze the pretrained hidden layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the softmax output layer\n",
    "pretrained_model.output = nn.Linear(hidden_size, new_output_size)\n",
    "\n",
    "\n",
    "# Set up data loaders for digits 5 to 9 with limited images\n",
    "indices = [i for i, (_, label) in enumerate(train_dataset) if label >= 5]\n",
    "limited_indices = np.random.choice(indices, size=(500,), replace=False)\n",
    "train_sampler = SubsetRandomSampler(limited_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "\n",
    "# Modify the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.view(-1, input_size))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "# c. To cache the frozen layers and train the model again, you can make use of the torch.no_grad() context manager to disable gradient \n",
    "# computation and cache the outputs of the frozen layers. This avoids unnecessary computations and speeds up the training process.\n",
    "\n",
    "# d. To reuse only four hidden layers instead of five, you can modify the pretrained model by removing the last hidden layer. However, \n",
    "# achieving higher precision solely by reducing the number of hidden layers may not be guaranteed, as the model's performance depends on \n",
    "# various factors such as the complexity of the task and the available training data.\n",
    "\n",
    "# e. To unfreeze the top two hidden layers and continue training, you can selectively enable gradient computation for those layers by \n",
    "# setting requires_grad = True. This allows the unfrozen layers to update their weights during training, potentially improving the \n",
    "# model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3cac4-d3d8-44c5-9e50-358aabbcb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pretraining on an auxiliary task.\n",
    "# a. In this exercise you will build a DNN that compares two MNIST digit images and\n",
    "# predicts whether they represent the same digit or not. Then you will reuse the lower\n",
    "# layers of this network to train an MNIST classifier using very little training data. Start\n",
    "# by building two DNNs (let’s call them DNN A and B), both similar to the one you built\n",
    "# earlier but without the output layer: each DNN should have five hidden layers of 100\n",
    "# neurons each, He initialization, and ELU activation. Next, add one more hidden layer\n",
    "# with 10 units on top of both DNNs. To do this, you should use\n",
    "# TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs\n",
    "# for each instance, then feed the result to the hidden layer. Finally, add an output\n",
    "# layer with a single neuron using the logistic activation function.\n",
    "# b. Split the MNIST training set in two sets: split #1 should containing 55,000 images,\n",
    "# and split #2 should contain contain 5,000 images. Create a function that generates a\n",
    "# training batch where each instance is a pair of MNIST images picked from split #1.\n",
    "# Half of the training instances should be pairs of images that belong to the same\n",
    "# class, while the other half should be images from different classes. For each pair, the\n",
    "\n",
    "# training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
    "# c. Train the DNN on this training set. For each image pair, you can simultaneously feed\n",
    "# the first image to DNN A and the second image to DNN B. The whole network will\n",
    "# gradually learn to tell whether two images belong to the same class or not.\n",
    "# d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and\n",
    "# adding a softmax output layer on top with 10 neurons. Train this network on split #2\n",
    "# and see if you can achieve high performance despite having only 500\n",
    "\n",
    "# Sol:\n",
    "\n",
    "# Build DNN A\n",
    "dnn_a = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(input_size,)),\n",
    "    # Add four more hidden layers\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal')\n",
    "])\n",
    "\n",
    "# Build DNN B (similar to DNN A)\n",
    "dnn_b = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(input_size,)),\n",
    "    # Add four more hidden layers\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal')\n",
    "])\n",
    "\n",
    "# Concatenate outputs of DNN A and DNN B\n",
    "concat = tf.keras.layers.Concatenate(axis=1)([dnn_a.output, dnn_b.output])\n",
    "\n",
    "# Add additional hidden layer\n",
    "hidden = tf.keras.layers.Dense(10, activation='elu', kernel_initializer='he_normal')(concat)\n",
    "\n",
    "# Add output layer with logistic activation\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.models.Model(inputs=[dnn_a.input, dnn_b.input], outputs=output)\n",
    "\n",
    "\n",
    "def generate_training_batch(split1_images, split1_labels, batch_size):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for _ in range(batch_size // 2):\n",
    "        # Select two random indices from split #1\n",
    "        idx1, idx2 = np.random.choice(len(split1_images), size=2, replace=False)\n",
    "        \n",
    "        # Append two images to the batch\n",
    "        batch_images.append([split1_images[idx1], split1_images[idx2]])\n",
    "        \n",
    "        # Check if the images belong to the same class\n",
    "        if split1_labels[idx1] == split1_labels[idx2]:\n",
    "            # Images belong to the same class, label = 0\n",
    "            batch_labels.append(0)\n",
    "        else:\n",
    "            # Images belong to different classes, label = 1\n",
    "            batch_labels.append(1)\n",
    "\n",
    "    return np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(num_batches):\n",
    "        # Generate a training batch\n",
    "        batch_images, batch_labels = generate_training_batch(split1_images, split1_labels, batch_size)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            output = model(batch_images)\n",
    "            loss = loss_fn(batch_labels, output)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(num_batches):\n",
    "        # Generate a training batch\n",
    "        batch_images, batch_labels = generate_training_batch(split1_images, split1_labels, batch_size)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            output = model(batch_images)\n",
    "            loss = loss_fn(batch_labels, output)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6469cb-e897-475b-a01e-6fb88dcbde56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
