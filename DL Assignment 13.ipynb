{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1bed2e-c36c-47b5-9501-1d8b0481db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "# Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "# algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "# Ans:\n",
    "# It is generally preferable to use a Logistic Regression classifier instead of a classical Perceptron because Logistic Regression \n",
    "# provides probabilistic outputs and can handle linearly non-separable data.\n",
    "\n",
    "# A Perceptron trained using the Perceptron training algorithm can only classify linearly separable data, while Logistic Regression can \n",
    "# handle linearly non-separable data by modeling the probability of class membership. Logistic Regression uses a sigmoid function to \n",
    "# transform the output into a probability, allowing for more flexible decision boundaries.\n",
    "\n",
    "# To make a Perceptron equivalent to a Logistic Regression classifier, the activation function can be replaced with a sigmoid function,\n",
    "# such as the logistic function. By incorporating the sigmoid activation, the Perceptron can produce probabilistic outputs similar to\n",
    "# Logistic Regression. Additionally, the Perceptron's weight update rule can be modified to use gradient descent with the cross-entropy\n",
    "# loss, which aligns it with the maximum likelihood estimation used in Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45365d25-7365-4b63-95f2-a7468852c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "\n",
    "# Ans:\n",
    "# The logistic activation function, also known as the sigmoid function, was a key ingredient in training the first Multilayer Perceptrons\n",
    "# (MLPs) because it enabled the models to learn complex non-linear relationships.\n",
    "\n",
    "# The logistic activation function has a smooth and differentiable nature, which allowed gradients to flow through the network during \n",
    "# backpropagation. This facilitated efficient optimization using gradient-based methods, such as gradient descent.\n",
    "\n",
    "# By introducing non-linearity through the logistic activation function, MLPs became capable of approximating complex functions and\n",
    "# capturing intricate patterns in the data. This made them more powerful and expressive compared to the linear models that preceded them, \n",
    "# enabling them to solve a wider range of real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f17bd5-2d71-4b2d-b68d-11143609e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Name three popular activation functions. Can you draw them?\n",
    "\n",
    "# Ans:\n",
    "# Sigmoid (Logistic) Activation Function: The sigmoid function squeezes the input into a range between 0 and 1. It has an S-shaped curve, \n",
    "# and its output represents a probability-like value. It is useful in binary classification problems and the intermediate layers of \n",
    "# neural networks.\n",
    "\n",
    "# Rectified Linear Unit (ReLU) Activation Function: ReLU is a simple yet effective activation function that sets negative inputs to zero \n",
    "# and keeps positive inputs unchanged. It introduces non-linearity and sparsity into the network, making it computationally efficient\n",
    "# and effective in deep neural networks.\n",
    "\n",
    "# Hyperbolic Tangent (Tanh) Activation Function: The tanh function is similar to the sigmoid function but maps the input to a range \n",
    "# between -1 and 1. It is symmetric around the origin and has steeper gradients compared to the sigmoid function. Tanh is commonly\n",
    "# used in recurrent neural networks (RNNs) and can be effective in capturing both positive and negative input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd021f9-e362-4d5b-98dd-65b0633c8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "# followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "# artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "#  What is the shape of the input matrix X?\n",
    "#  What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "# bias vector bh?\n",
    "#  What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "#  What is the shape of the network’s output matrix Y?\n",
    "#  Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo.\n",
    "\n",
    "# Ans:\n",
    "# The shape of the input matrix X would be (batch_size, 10), where batch_size represents the number of samples in a batch.\n",
    "\n",
    "# The shape of the hidden layer's weight vector Wh would be (10, 50), and the shape of its bias vector bh would be (50,).\n",
    "\n",
    "# The shape of the output layer's weight vector Wo would be (50, 3), and the shape of its bias vector bo would be (3,).\n",
    "\n",
    "# The shape of the network's output matrix Y would be (batch_size, 3), where batch_size represents the number of samples in a batch and \n",
    "# 3 represents the number of output neurons in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e5b54a-e899-499a-a46c-eea42cd83720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "# or ham? What activation function should you use in the output layer? If instead you want to\n",
    "# tackle MNIST, how many neurons do you need in the output layer, using what activation function?\n",
    "\n",
    "# Ans:\n",
    "# For classifying email into spam or ham, you would need 2 neurons in the output layer. The activation function typically used in the\n",
    "# output layer for binary classification tasks like this is the sigmoid function, which outputs a probability value between 0 and 1 \n",
    "# representing the likelihood of each class.\n",
    "\n",
    "# For tackling the MNIST dataset, you would need 10 neurons in the output layer. MNIST is a multi-class classification problem with 10 \n",
    "# distinct digits (0 to 9). The activation function commonly used in the output layer for multi-class classification tasks like this \n",
    "# is the softmax function. The softmax function calculates the probabilities of each class and ensures that the sum of the probabilities \n",
    "# adds up to 1, allowing us to interpret the output as class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251e6f0c-cd47-43a6-aaeb-6394bf9eeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "# Ans:\n",
    "# Backpropagation is a computational technique used to calculate gradients in a neural network during the training process.\n",
    "# It works by propagating the error or loss backward through the network, updating the weights and biases of each layer based on the \n",
    "# computed gradients using the chain rule of calculus.\n",
    "\n",
    "# Backpropagation involves two main steps: forward pass and backward pass. During the forward pass, input data is propagated through the \n",
    "# network, and intermediate activations are computed. Then, during the backward pass, the error is calculated by comparing the network's \n",
    "# output with the desired output. Gradients are then computed layer by layer, starting from the output layer and moving backward, using \n",
    "# the chain rule to update the weights and biases.\n",
    "\n",
    "# Reverse-mode autodiff, also known as reverse-mode differentiation or backpropagation through time (BPTT), is a specific implementation \n",
    "# of the backpropagation algorithm. It refers to the process of computing gradients efficiently in neural networks by exploiting the chain\n",
    "# rule and reusing intermediate computations in reverse order.\n",
    "\n",
    "# In essence, backpropagation is a general concept that refers to the computation of gradients using the chain rule, while reverse-mode\n",
    "# autodiff is a specific algorithmic implementation of backpropagation that is commonly used in deep learning and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c99d53e-76f3-4f49-a0c3-cdbebed5fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "# training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "# Ans:\n",
    "# ome of the hyperparameters that can be tweaked in an MLP (Multilayer Perceptron) include:\n",
    "\n",
    "# Number of hidden layers and their sizes.\n",
    "# Activation function used in each layer.\n",
    "# Learning rate.\n",
    "# Batch size.\n",
    "# Number of training epochs.\n",
    "# Regularization techniques (e.g., L1 or L2 regularization).\n",
    "# Dropout rate.\n",
    "# Weight initialization strategy.\n",
    "# Optimizer algorithm (e.g., Adam, SGD).\n",
    "# Early stopping criteria.\n",
    "# If the MLP is overfitting the training data, several hyperparameters can be adjusted to address the issue:\n",
    "\n",
    "# Decrease the size or number of hidden layers to reduce model complexity.\n",
    "# Increase the regularization strength or apply different regularization techniques to prevent overfitting.\n",
    "# Adjust the dropout rate to improve generalization.\n",
    "# Increase the batch size to introduce more randomness during training.\n",
    "# Reduce the learning rate to slow down the learning process and allow the model to converge to a better solution.\n",
    "# Introduce early stopping by monitoring a validation set and stopping training when the performance starts to deteriorate.\n",
    "# Collect more training data if possible to provide a diverse and representative sample.\n",
    "# These adjustments aim to balance the model's capacity to learn complex patterns while avoiding overfitting, promoting better\n",
    "# generalization to unseen data. The specific hyperparameters to tweak may vary depending on the problem, dataset, and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c710931a-9b8d-4803-b2e9-5dafa1b9a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
    "# adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
    "# an interruption, add summaries, plot learning curves using TensorBoard, and so on).\n",
    "\n",
    "# Sol:\n",
    "\n",
    "# Import the necessary libraries and load the MNIST dataset.\n",
    "# reprocess the data by scaling the pixel values and performing one-hot encoding on the labels.\n",
    "# Define the MLP model architecture, including the number of layers, their sizes, and activation functions.\n",
    "# Set up the necessary components for training, such as loss function, optimizer, and evaluation metrics.\n",
    "# Implement the training loop, which involves forward and backward passes, weight updates, and monitoring performance.\n",
    "# Add features like checkpoint saving and restoration to handle interruptions and resume training.\n",
    "# Implement tensorboard logging to track and visualize the learning curves and performance metrics.\n",
    "# Train the MLP model using the prepared data and evaluate its performance on the test set.\n",
    "# Continuously monitor the learning curves and make adjustments to hyperparameters if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65683f61-9a3f-4247-a128-2564f85ab7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
