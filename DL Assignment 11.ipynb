{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d930c5a-3a27-40a5-8949-8ee8f2ce8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.6984762140113583\n"
     ]
    }
   ],
   "source": [
    "# 1. Write the Python code to implement a single neuron.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weights = np.random.rand(num_inputs)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "    def activate(self, inputs):\n",
    "        # Weighted sum of inputs\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        # Activation function (in this case, using a sigmoid function)\n",
    "        activation = 1 / (1 + np.exp(-weighted_sum))\n",
    "        return activation\n",
    "\n",
    "# Example usage:\n",
    "neuron = Neuron(3)  # Creating a neuron with 3 inputs\n",
    "inputs = np.array([0.5, 0.3, 0.8])  # Example input values\n",
    "output = neuron.activate(inputs)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9506c668-b831-4dad-9731-0f8f2f6d876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0 2 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "# 2. Write the Python code to implement ReLU.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage:\n",
    "input_data = np.array([-1, 2, -3, 4, -5])\n",
    "output = relu(input_data)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600693ed-6d6b-45e2-ac93-216c43fb89fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.95114127 0.8029199 ]\n"
     ]
    }
   ],
   "source": [
    "# 3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Perform matrix multiplication between inputs and weights\n",
    "        weighted_sum = np.dot(inputs, self.weights)\n",
    "        # Add the bias term\n",
    "        weighted_sum += self.bias\n",
    "        return weighted_sum\n",
    "\n",
    "# Example usage:\n",
    "dense_layer = DenseLayer(3, 2)  # Creating a dense layer with 3 inputs and 2 outputs\n",
    "inputs = np.array([0.5, 0.3, 0.8])  # Example input values\n",
    "output = dense_layer.forward(inputs)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb4ef57-054a-4119-8850-f1018235f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [1.6235156756147706, 1.2515894829921255]\n"
     ]
    }
   ],
   "source": [
    "# 4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "# and functionality built into Python).\n",
    "\n",
    "# Sol:\n",
    "\n",
    "import random\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = []\n",
    "        for _ in range(output_size):\n",
    "            neuron_weights = []\n",
    "            for _ in range(input_size):\n",
    "                neuron_weights.append(random.random())\n",
    "            self.weights.append(neuron_weights)\n",
    "\n",
    "        self.bias = []\n",
    "        for _ in range(output_size):\n",
    "            self.bias.append(random.random())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = []\n",
    "        for i in range(len(self.bias)):\n",
    "            neuron_sum = 0\n",
    "            for j in range(len(inputs)):\n",
    "                neuron_sum += inputs[j] * self.weights[i][j]\n",
    "            neuron_sum += self.bias[i]\n",
    "            weighted_sum.append(neuron_sum)\n",
    "        return weighted_sum\n",
    "\n",
    "# Example usage:\n",
    "dense_layer = DenseLayer(3, 2)  # Creating a dense layer with 3 inputs and 2 outputs\n",
    "inputs = [0.5, 0.3, 0.8]  # Example input values\n",
    "output = dense_layer.forward(inputs)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b6c74b-af4a-4d4d-8191-3338fdcd89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What is the “hidden size” of a layer?\n",
    "\n",
    "# Ans:\n",
    "# The \"hidden size\" of a layer refers to the number of neurons or units present in that particular layer. In a neural network, \n",
    "# the hidden layers are the layers situated between the input layer and the output layer. Each hidden layer consists of multiple neurons,\n",
    "# and the hidden size specifies the number of neurons in that layer.\n",
    "\n",
    "# The hidden size is a hyperparameter that can be adjusted when designing a neural network. It determines the capacity and complexity of\n",
    "# the model. Increasing the hidden size allows the layer to capture more intricate patterns in the data but also leads to a higher number \n",
    "# of parameters, potentially increasing the risk of overfitting. On the other hand, reducing the hidden size decreases the model's \n",
    "# capacity to learn complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ca0b03-0a63-49a8-abd5-0016b298d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What does the t method do in PyTorch?\n",
    "\n",
    "# Ans:\n",
    "# In PyTorch, the t method is used to perform the transpose operation on a tensor. It returns a new tensor that is a transposed version\n",
    "# of the original tensor, swapping the dimensions or axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69200ab8-7ac0-41ca-84fa-94d06976de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "# Ans:\n",
    "# Lack of vectorization: In plain Python, matrix multiplication is typically implemented using nested loops, which can be computationally\n",
    "# inefficient. The lack of vectorized operations in plain Python leads to slower execution compared to libraries that utilize optimized,\n",
    "# low-level implementations.\n",
    "\n",
    "# Dynamic typing: Python is dynamically typed, which means that the type of an object can change during runtime. This flexibility comes \n",
    "# with a performance cost. In contrast, libraries like NumPy and PyTorch use static typing and often leverage compiled code, resulting \n",
    "# in faster matrix multiplication operations.\n",
    "\n",
    "# Optimizations and low-level operations: Libraries like NumPy and PyTorch are built with optimized matrix multiplication algorithms \n",
    "# that take advantage of low-level operations such as CPU parallelism, cache utilization, and optimized memory access patterns. \n",
    "# These optimizations significantly enhance the performance of matrix multiplication operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83199080-ab3f-486e-941a-e34accaecb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. In matmul, why is ac==br?\n",
    "\n",
    "# Ans:\n",
    "# In matrix multiplication, the requirement that the number of columns in the first matrix (a) must be equal to the number of rows \n",
    "# in the second matrix (b) (ac == br) ensures that the matrices are compatible for multiplication. This condition ensures that the\n",
    "# inner dimensions of the matrices match, allowing for the dot product to be computed between corresponding elements of the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387923b2-dc4b-4305-b6c9-83befd82af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "# Ans:\n",
    "# In Jupyter Notebook, you can measure the time taken for a single cell to execute using the %timeit magic command. \n",
    "# Simply prefix the cell code with %timeit, and Jupyter Notebook will execute the cell multiple times to get an average execution time.\n",
    "\n",
    "# For example:\n",
    "\n",
    "# %timeit my_function() - Run it in the terminal\n",
    "\n",
    "# This will execute the my_function() and provide you with the average execution time over multiple runs. The output will \n",
    "# include the execution time and the number of loops performed.\n",
    "\n",
    "# Alternatively, you can use the %time magic command to measure the time taken for a single execution of a cell without averaging over \n",
    "# multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0487983-992a-420e-bfbc-bbbbf06a319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. What is elementwise arithmetic?\n",
    "\n",
    "# Ans:\n",
    "# Elementwise arithmetic refers to performing arithmetic operations on corresponding elements of two or more arrays or tensors.\n",
    "# In elementwise arithmetic, the operation is applied individually to each pair of corresponding elements in the input arrays, \n",
    "# resulting in a new array or tensor of the same shape as the inputs.\n",
    "\n",
    "# For example, in elementwise addition, each element of the first array is added to the corresponding element of the second array to\n",
    "# produce a new array with the same shape. The same concept applies to subtraction, multiplication, division, and other arithmetic\n",
    "# operations.\n",
    "\n",
    "# Elementwise arithmetic is a fundamental operation in many mathematical and scientific computations and is efficiently implemented \n",
    "# in libraries like NumPy, PyTorch, and TensorFlow, enabling efficient computations on arrays and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658057cd-9204-4414-a984-3781e51e8951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.7)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.0)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.3.tar.gz (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.3-py3-none-any.whl size=88175 sha256=293717a6fbfde866047b2d9e9148bf8a9b9b2f3b9b82ed3ee311f4e831755a94\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/d4/a9/7e/a0edb2716869ea46a687b1f7854e3e405c114d6611dc709b68\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.3 filelock-3.12.0 lit-16.0.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fde9c9b-c461-495c-932d-5bb4bc051ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# 11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
    "\n",
    "# Ans:\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 1, 2])\n",
    "\n",
    "result = torch.all(a > b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddbc966d-f68e-4150-a9ee-7bf0cc601776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "# Ans:\n",
    "# A rank-0 tensor, also known as a scalar tensor, is a tensor with zero dimensions. It represents a single value in a tensor.\n",
    "\n",
    "# To convert a rank-0 tensor to a plain Python data type, you can use the .item() method. The .item() method retrieves the value from\n",
    "# the rank-0 tensor and returns it as a standard Python data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeb8a20f-364f-453c-8780-dbf50df7e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "# Ans:\n",
    "# Elementwise arithmetic does not directly speed up matrix multiplication (matmul) itself. However, it can be used to\n",
    "# optimize certain operations within the matmul computation.\n",
    "\n",
    "# During matrix multiplication, elementwise arithmetic operations such as addition and multiplication are performed between \n",
    "# corresponding elements of the matrices being multiplied. These elementwise operations are typically efficiently implemented in \n",
    "# optimized libraries or hardware.\n",
    "\n",
    "# By leveraging optimized elementwise arithmetic operations, libraries like NumPy, PyTorch, and TensorFlow can accelerate the matrix\n",
    "# multiplication process. These libraries utilize optimized low-level implementations, parallelism, and efficient memory access\n",
    "# patterns to perform elementwise operations efficiently, thereby speeding up the overall matrix multiplication computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8882914d-1d55-4c51-ade2-832b2093f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. What are the broadcasting rules?\n",
    "\n",
    "# Ans:\n",
    "# Broadcasting rules are rules that determine how arrays of different shapes are treated during elementwise operations in NumPy \n",
    "# and other array-based libraries. The broadcasting rules enable these libraries to perform operations between arrays with different\n",
    "# shapes by implicitly extending or replicating their values.\n",
    "\n",
    "# The broadcasting rules are as follows:\n",
    "\n",
    "# If the arrays have the same shape, the elementwise operation is applied directly.\n",
    "# If the arrays have different shapes, the array with fewer dimensions is \"stretched\" or \"broadcasted\" along the missing dimensions\n",
    "# to match the shape of the other array.\n",
    "# If the size of a particular dimension is 1 in one of the arrays, it is expanded to match the corresponding dimension's size in the\n",
    "# other array.\n",
    "# If the arrays have incompatible shapes and cannot be broadcasted, an error is raised.\n",
    "# By following these broadcasting rules, arrays of different shapes can participate in elementwise operations without the need for\n",
    "# explicit resizing or looping over the arrays. This allows for more concise and efficient code when working with arrays of different\n",
    "# shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b26d9969-ac58-4be9-a30b-e63fdeaada40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "# Ans:\n",
    "# The expand_as method in PyTorch is used to expand the shape of a tensor to match the shape of another tensor. It is commonly used to \n",
    "# align tensors for elementwise operations or broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0211bd-9202-432d-8de9-1b00fe2ee502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
